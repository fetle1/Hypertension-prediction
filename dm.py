# -*- coding: utf-8 -*-
"""DM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5V0h2GxVL_FsYWBMKv37hwwQ1ElvAIg
"""

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/Hypertension_Edited.csv')

sex_mapping = {
    'Women': 1,
    '1.0': 1,
    'Men': 0,
    '0.0': 0
}

# Apply the mapping to the 'sex' column
# .loc is used for clear intent when modifying the DataFrame
df.loc[:, 'sex'] = df['sex'].map(sex_mapping)

# Optional: Verify the changes
print("\nUnique values in 'sex' column after mapping:")
print(df['sex'].unique())
print("\nData type of 'sex' column after mapping:")
print(df['sex'].dtype)

import numpy as np

# Create the new 'DM' column
df['DM'] = np.where(df['fbs_category'] == 'Hyperglycemia', 'yes', 'No')

# Display the first few rows to check the new column
print(df[['fbs_category', 'DM']].head())

df = df.drop(columns=['psu'])

df = df.drop(columns=['fbs_category'])

df.to_csv('/content/drive/MyDrive/DM.csv', index=False)

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/DM.csv')

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np # Import numpy as it's used

# Assuming your full DataFrame is named df
# Replace with actual DataFrame name if different

# 1. Preprocess the data
# It's good practice to handle NaNs before encoding
df = df.dropna()

# Convert categorical variables to numeric using LabelEncoder
# Identify columns with 'object' dtype *after* dropping NaNs if needed
categorical_cols = df.select_dtypes(include='object').columns

# Apply Label Encoding only to actual object columns, including the target variable 'DM'
for col in categorical_cols:
    # Removed the condition 'if col != 'DM':' so that 'DM' is also label encoded
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])


# Set up features and target
# Ensure 'DM' is handled correctly. Since it was an object type and is now label encoded,
# it should already be numerical (0 or 1).
X = df.drop(columns=['DM'])  # Features
y = df['DM']     # 'DM' is now numerical after label encoding, no need for .astype(int)

# Ensure column names are valid after preprocessing if needed (LabelEncoder doesn't change names)
# X.columns = ["".join (c if c.isalnum() else "_" for c in str(x)) for x in X.columns] # This might not be necessary with LabelEncoder

# 2. Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. Train Random Forest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# 4. Feature importances
importances = rf_model.feature_importances_
# REGENERATE feature_names and indices based on X_train *within this cell*
feature_names = X_train.columns
indices = np.argsort(importances) # Sort in ascending order

# 5. DataFrame for visualization
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# 6. Plot - Use the variables from step 4 and 5
plt.figure(figsize=(10, 6))
# Create color gradient based on the correct number of features
colors = plt.cm.Greens(np.linspace(0.3, 1, len(feature_names)))[indices] # Use feature_names length

plt.barh(range(len(feature_names)), importances[indices], color=colors) # Use feature_names length
plt.yticks(range(len(feature_names)), [feature_names[i] for i in indices]) # Use feature_names and indices
plt.tight_layout()

# You had a duplicate plotting command here that uses the importance_df
# Let's remove the duplicate and use the one based on importance_df as it's cleaner
# plt.barh(importance_df['Feature'], importance_df['Importance']) # Removed this duplicate line
# The barh command above already plots importance_df['Feature'] vs importance_df['Importance']
# because importance_df is sorted by importance, and plotting based on indices gives the same order.

plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title("Feature Importance from Random Forest")
plt.gca().invert_yaxis() # Invert y-axis to have most important feature at the top
plt.show()

# 7. Print
print(importance_df)

# %%
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np # Import numpy as it's used

# Assuming your full DataFrame is named df
# Replace with actual DataFrame name if different

# 1. Preprocess the data
# It's good practice to handle NaNs before encoding
df = df.dropna()

# Convert categorical variables to numeric using LabelEncoder
# Identify columns with 'object' dtype *after* dropping NaNs if needed
categorical_cols = df.select_dtypes(include='object').columns

# Apply Label Encoding only to actual object columns, exclude the target variable 'DM'
for col in categorical_cols:
    if col != 'DM': # Ensure 'DM' is not label encoded if it's already handled as the target
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])


# Set up features and target
# Ensure 'DM' is handled correctly. If it was one of the columns in categorical_cols
# it should now be numerical after the loop above.
# If 'DM' wasn't an object type but needs to be integer, keep the .astype(int)
X = df.drop(columns=['DM'])  # Features
y = df['DM'].astype(int)     # Make sure it's categorical (int)

# Ensure column names are valid after preprocessing if needed (LabelEncoder doesn't change names)
# X.columns = ["".join (c if c.isalnum() else "_" for c in str(x)) for x in X.columns] # This might not be necessary with LabelEncoder

# 2. Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Print original class distribution
print("Original training data shape:", X_train.shape)
print("Original class distribution:", y_train.value_counts())

# %%
# Install imbalanced-learn if you haven't already
# !pip install imbalanced-learn

from imblearn.over_sampling import SMOTE

# Apply SMOTE to the training data only
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Print the class distribution after SMOTE
print("\nTraining data shape after SMOTE:", X_train_smote.shape)
print("Class distribution after SMOTE:", y_train_smote.value_counts())

# %%
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier # Include RF again
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score
import pandas as pd

# Define the models to train
models = {
    "Logistic Regression": LogisticRegression(random_state=42, solver='liblinear'), # Added solver for older versions
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Gaussian Naive Bayes": GaussianNB(),
    "Support Vector Machine (Linear)": SVC(kernel='linear', random_state=42, probability=True), # probability=True for AUC
    "Support Vector Machine (RBF)": SVC(kernel='rbf', random_state=42, probability=True), # probability=True for AUC
    "MLP Classifier": MLPClassifier(random_state=42, max_iter=1000), # Increased max_iter
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "AdaBoost Classifier": AdaBoostClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42) # Random Forest from previous step
}

# DataFrame to store results
results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'])

# Iterate through models, train, predict, and evaluate
for model_name, model in models.items():
    print(f"Training {model_name}...")

    # Train the model on SMOTE-augmented data
    model.fit(X_train_smote, y_train_smote)

    # Predict on the original test set
    y_pred = model.predict(X_test)

    # Get probability scores for AUC (only for models that support it)
    if hasattr(model, "predict_proba"):
        y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class
    else:
        y_pred_proba = None # Some models like SVC might need probability=True

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    # Handle cases where y_pred_proba is None gracefully for AUC
    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None and len(np.unique(y_test)) > 1 else 'N/A'


    # Create a new DataFrame for the current row of results
    new_row = pd.DataFrame([{
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'AUC': auc
    }])

    # Use pandas.concat to add the new row to the results_df
    results_df = pd.concat([results_df, new_row], ignore_index=True)


# Print the results table
print("\n--- Model Performance Summary ---")
print(results_df.to_markdown(index=False)) # Use to_markdown for nice formatting

!pip install imbalanced-learn scikit-learn pandas

# Install imbalanced-learn if you haven't already
!pip install imbalanced-learn scikit-learn pandas

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score, confusion_matrix, balanced_accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE # Assuming you've already run the SMOTE step

# Assuming X_train_smote, y_train_smote, X_test, y_test are already defined from your previous steps

# Define the models to train
# Added class_weight='balanced' to some models that support it
models = {
    "Logistic Regression": LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced'),
    "Decision Tree": DecisionTreeClassifier(random_state=42, class_weight='balanced'),
    "K-Nearest Neighbors": KNeighborsClassifier(), # KNN doesn't have class_weight
    "Gaussian Naive Bayes": GaussianNB(), # Naive Bayes doesn't have class_weight
    "MLP Classifier": MLPClassifier(random_state=42, max_iter=1000), # MLP doesn't have class_weight (directly, can be done via sample_weight)
    "Gradient Boosting": GradientBoostingClassifier(random_state=42), # GB doesn't have class_weight (can be done via sample_weight)
    "AdaBoost Classifier": AdaBoostClassifier(random_state=42), # AdaBoost doesn't have class_weight
    "Random Forest": RandomForestClassifier(random_state=42, class_weight='balanced') # Added class_weight='balanced'
}

# DataFrame to store results, including balanced accuracy
results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'])

# Iterate through models, train, predict, and evaluate
for model_name, model in models.items():
    print(f"Training {model_name}...")

    # Train the model on SMOTE-augmented data (or use original training data with class_weight)
    # Using SMOTE + class_weight can sometimes be beneficial, but try both!
    model.fit(X_train_smote, y_train_smote) # Using SMOTE data

    # Predict on the original test set
    y_pred = model.predict(X_test)

    # Get probability scores for AUC and threshold adjustment
    if hasattr(model, "predict_proba"):
        y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class
    else:
        y_pred_proba = None

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0) # Added zero_division to handle cases with no positive predictions
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, zero_division=0) # Added zero_division

    # Handle cases where y_pred_proba is None or only one class is present in y_test for AUC
    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None and len(np.unique(y_test)) > 1 else 'N/A'


    # Create a new DataFrame for the current row of results
    new_row = pd.DataFrame([{
        'Model': model_name,
        'Accuracy': accuracy,
        'Balanced Accuracy': balanced_acc,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'AUC': auc
    }])

    # Use pandas.concat to add the new row to the results_df
    results_df = pd.concat([results_df, new_row], ignore_index=True)

# Print the results table
print("\n--- Model Performance Summary ---")
print(results_df.to_markdown(index=False))

# --- Example of evaluating a single model with a different threshold ---
print("\n--- Evaluating Logistic Regression with Adjusted Threshold ---")
# Retrain or use the already trained Logistic Regression model
lr_model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')
lr_model.fit(X_train_smote, y_train_smote)

# Get probability predictions
y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]

# Choose a different threshold (e.g., 0.3)
threshold = 0.3
y_pred_adjusted = (y_pred_proba_lr >= threshold).astype(int)

# Calculate metrics with adjusted threshold
accuracy_adj = accuracy_score(y_test, y_pred_adjusted)
balanced_acc_adj = balanced_accuracy_score(y_test, y_pred_adjusted)
precision_adj = precision_score(y_test, y_pred_adjusted, zero_division=0)
recall_adj = recall_score(y_test, y_pred_adjusted)
f1_adj = f1_score(y_test, y_pred_adjusted, zero_division=0)

print(f"Threshold: {threshold}")
print(f"Accuracy: {accuracy_adj:.4f}")
print(f"Balanced Accuracy: {balanced_acc_adj:.4f}")
print(f"Precision: {precision_adj:.4f}")
print(f"Recall: {recall_adj:.4f}")
print(f"F1 Score: {f1_adj:.4f}")

# You can repeat this for other models or loop through different thresholds

# --- Install necessary libraries ---
# Uncomment this line if running for the first time
# !pip install imbalanced-learn scikit-learn pandas matplotlib xgboost

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier
from sklearn.metrics import (
    precision_score, recall_score, accuracy_score, f1_score,
    roc_auc_score, confusion_matrix, balanced_accuracy_score,
    precision_recall_curve
)
from sklearn.model_selection import train_test_split
from imblearn.combine import SMOTEENN
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier

# --- Replace this with your actual data loading ---
# Assuming df is already loaded and preprocessed (e.g., sex, DM, psu, fbs_category handled)

# Separate features and target
X = df.drop('DM', axis=1)
y = df['DM']

# Encode categorical features in X
categorical_cols_in_X = X.select_dtypes(include='object').columns
X_encoded = X.copy()

for col in categorical_cols_in_X:
    le = LabelEncoder()
    X_encoded.loc[:, col] = le.fit_transform(X_encoded[col])


# --- Apply SMOTEENN to the entire dataset ---
smote_enn = SMOTEENN(random_state=42)
X_resampled_enn, y_resampled_enn = smote_enn.fit_resample(X_encoded, y) # Use X_encoded

# --- Split the balanced data ---
# Now split the resampled data into train and test
# Stratify by y_resampled_enn to maintain class distribution in the split
X_train, X_test, y_train, y_test = train_test_split(X_resampled_enn, y_resampled_enn, test_size=0.2, random_state=42, stratify=y_resampled_enn)


# --- Define classifiers ---
models = {
    "Logistic Regression": LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced'),
    "Decision Tree": DecisionTreeClassifier(random_state=42, class_weight='balanced'),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Gaussian Naive Bayes": GaussianNB(),
    "MLP Classifier": MLPClassifier(random_state=42, max_iter=1000),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "AdaBoost Classifier": AdaBoostClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42, class_weight='balanced'),
    "XGBoost": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss',
                              # Use y_train (from the SMOTEENN split) to calculate scale_pos_weight
                              scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train))
}

# DataFrame to store results
results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'])

# --- Train and evaluate each model ---
for model_name, model in models.items():
    print(f"Training {model_name}...")
    # Train on the resampled training data (X_train, y_train)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    accuracy = accuracy_score(y_test, y_pred)
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None and len(np.unique(y_test)) > 1 else 'N/A'

    results_df = pd.concat([results_df, pd.DataFrame([{
        'Model': model_name,
        'Accuracy': accuracy,
        'Balanced Accuracy': balanced_acc,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'AUC': auc
    }])], ignore_index=True)

# --- Show full results table ---
print("\n--- Model Performance Summary ---")
print(results_df.to_markdown(index=False))

# --- Threshold tuning for Logistic Regression ---
print("\n--- Logistic Regression: Threshold Tuning (Precision ≈ Recall) ---")
# Retrain the Logistic Regression model on the resampled training data
lr_model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')
lr_model.fit(X_train, y_train)
y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]

precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba_lr)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)

# Filter for thresholds where precision is at most 15% greater than recall
# Added a check for empty valid_indices
if valid_indices:
    best_idx = max(valid_indices, key=lambda i: f1_scores[i])
else:
    best_idx = np.argmax(f1_scores) # Fallback to max F1 if no threshold meets the criterion

best_threshold = thresholds[best_idx]

# Predict with best threshold
y_pred_adjusted = (y_pred_proba_lr >= best_threshold).astype(int)
acc_adj = accuracy_score(y_test, y_pred_adjusted)
bal_acc_adj = balanced_accuracy_score(y_test, y_pred_adjusted)
prec_adj = precision_score(y_test, y_pred_adjusted, zero_division=0)
recall_adj = recall_score(y_test, y_pred_adjusted)
f1_adj = f1_score(y_test, y_pred_adjusted, zero_division=0)

print(f"Best Threshold (Precision ≈ Recall): {best_threshold:.2f}")
print(f"Adjusted Accuracy: {acc_adj:.4f}")
print(f"Adjusted Balanced Accuracy: {bal_acc_adj:.4f}")
print(f"Adjusted Precision: {prec_adj:.4f}")
print(f"Adjusted Recall: {recall_adj:.4f}")
print(f"Adjusted F1 Score: {f1_adj:.4f}")

# --- Optional: Plot precision-recall trade-off ---
plt.figure(figsize=(8, 5))
plt.plot(thresholds, precisions[:-1], label='Precision')
plt.plot(thresholds, recalls[:-1], label='Recall')
plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Best Threshold ({best_threshold:.2f})')
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision, Recall & F1 vs Threshold")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import ConfusionMatrixDisplay

# --- Confusion Matrix for Best Performing Model: Random Forest ---
best_model_name = "Random Forest"
best_model = models[best_model_name]

# Predict on test set
y_pred_rf = best_model.predict(X_test)

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred_rf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)

# Plot
plt.figure(figsize=(6, 5))
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title(f"Confusion Matrix: {best_model_name}")
plt.grid(False)
plt.tight_layout()
plt.show()

# --- Install necessary libraries ---
# Uncomment this line if running for the first time
# !pip install imbalanced-learn scikit-learn pandas matplotlib xgboost

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier
from sklearn.metrics import (
    precision_score, recall_score, accuracy_score, f1_score,
    roc_auc_score, confusion_matrix, balanced_accuracy_score,
    precision_recall_curve
)
from sklearn.model_selection import train_test_split
from imblearn.combine import SMOTEENN
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier

# --- Replace this with your actual data loading ---
# Assuming df is already loaded and preprocessed (e.g., sex, DM, psu, fbs_category handled)

# Separate features and target
X = df.drop('DM', axis=1)
y = df['DM']

# Encode categorical features in X
categorical_cols_in_X = X.select_dtypes(include='object').columns
X_encoded = X.copy()

for col in categorical_cols_in_X:
    le = LabelEncoder()
    X_encoded.loc[:, col] = le.fit_transform(X_encoded[col])


# --- Apply SMOTEENN to the entire dataset ---
smote_enn = SMOTEENN(random_state=42)
X_resampled_enn, y_resampled_enn = smote_enn.fit_resample(X_encoded, y) # Use X_encoded

# --- Split the balanced data ---
# Now split the resampled data into train and test
# Stratify by y_resampled_enn to maintain class distribution in the split
X_train, X_test, y_train, y_test = train_test_split(X_resampled_enn, y_resampled_enn, test_size=0.2, random_state=42, stratify=y_resampled_enn)


# --- Define classifiers ---
models = {
    "Logistic Regression": LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced'),
    "Decision Tree": DecisionTreeClassifier(random_state=42, class_weight='balanced'),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Gaussian Naive Bayes": GaussianNB(),
    "MLP Classifier": MLPClassifier(random_state=42, max_iter=1000),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "AdaBoost Classifier": AdaBoostClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42, class_weight='balanced'),
    "XGBoost": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss',
                              # Use y_train (from the SMOTEENN split) to calculate scale_pos_weight
                              scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train))
}

# DataFrame to store results
results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'])

# --- Train and evaluate each model ---
for model_name, model in models.items():
    print(f"Training {model_name}...")
    # Train on the resampled training data (X_train, y_train)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    accuracy = accuracy_score(y_test, y_pred)
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None and len(np.unique(y_test)) > 1 else 'N/A'

    results_df = pd.concat([results_df, pd.DataFrame([{
        'Model': model_name,
        'Accuracy': accuracy,
        'Balanced Accuracy': balanced_acc,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'AUC': auc
    }])], ignore_index=True)

# --- Show full results table ---
print("\n--- Model Performance Summary ---")
print(results_df.to_markdown(index=False))

# --- Threshold tuning for Logistic Regression ---
print("\n--- Logistic Regression: Threshold Tuning (Precision ≈ Recall) ---")
# Retrain the Logistic Regression model on the resampled training data
lr_model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')
lr_model.fit(X_train, y_train)
y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]

precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba_lr)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)

# Define valid_indices: indices where precision is at most 15% greater than recall
# Use thresholds[:-1] because precisions and recalls arrays are one element longer than thresholds
valid_indices = [i for i, (p, r) in enumerate(zip(precisions[:-1], recalls[:-1])) if p <= r * 1.15]

# Filter for thresholds where precision is at most 15% greater than recall
# Added a check for empty valid_indices
if valid_indices:
    best_idx = max(valid_indices, key=lambda i: f1_scores[i])
else:
    best_idx = np.argmax(f1_scores) # Fallback to max F1 if no threshold meets the criterion

best_threshold = thresholds[best_idx]

# Predict with best threshold
y_pred_adjusted = (y_pred_proba_lr >= best_threshold).astype(int)
acc_adj = accuracy_score(y_test, y_pred_adjusted)
bal_acc_adj = balanced_accuracy_score(y_test, y_pred_adjusted)
prec_adj = precision_score(y_test, y_pred_adjusted, zero_division=0)
recall_adj = recall_score(y_test, y_pred_adjusted)
f1_adj = f1_score(y_test, y_pred_adjusted, zero_division=0)

print(f"Best Threshold (Precision ≈ Recall): {best_threshold:.2f}")
print(f"Adjusted Accuracy: {acc_adj:.4f}")
print(f"Adjusted Balanced Accuracy: {bal_acc_adj:.4f}")
print(f"Adjusted Precision: {prec_adj:.4f}")
print(f"Adjusted Recall: {recall_adj:.4f}")
print(f"Adjusted F1 Score: {f1_adj:.4f}")

# --- Optional: Plot precision-recall trade-off ---
plt.figure(figsize=(8, 5))
plt.plot(thresholds, precisions[:-1], label='Precision')
plt.plot(thresholds, recalls[:-1], label='Recall')
plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Best Threshold ({best_threshold:.2f})')
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision, Recall & F1 vs Threshold")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(10, 8))

# Loop through each model and plot its ROC curve
for model_name, model in models.items():
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        model_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {model_auc:.2f})')

# Plot the random chance line
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Chance')

plt.title('ROC Curves for All Models')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

pip install shap

import shap

# Use the trained XGBoost model (or replace with another model if needed)
best_model = models["Random Forest"]

# For tree-based models, use TreeExplainer
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test)

# Save SHAP values for later use
# Optionally: Save for sampling later
shap_values_array = np.array(shap_values)

# Create a SHAP summary beeswarm plot
shap.summary_plot(shap_values, X_test, plot_type="bee swarm", show=True)

import shap
import random
import numpy as np
import pandas as pd

# Sample 100 from X_test
X_test_sampled = X_test.sample(n=100, random_state=42)
sample_index = random.randint(0, 99)

# One instance to explain
selected_instance = X_test_sampled.iloc[sample_index:sample_index+1]

# Tree explainer for Random Forest
explainer_sample = shap.TreeExplainer(best_model)
shap_values_sample = explainer_sample.shap_values(selected_instance)

shap.initjs()

# Check if shap_values_sample is a list or array
if isinstance(shap_values_sample, list) and len(shap_values_sample) == 2:
    # Binary classification - use class 1 (positive)
    shap.force_plot(explainer_sample.expected_value[1],
                    shap_values_sample[1],
                    selected_instance)
else:
    # Single output case - use index 0
    shap.force_plot(explainer_sample.expected_value,
                    shap_values_sample,
                    selected_instance)

import shap
import random

# Select 100 samples and pick one randomly
X_test_sampled = X_test.sample(n=100, random_state=42)
sample_index = random.randint(0, 99)
selected_instance = X_test_sampled.iloc[sample_index:sample_index+1]

# Explain the selected instance
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(selected_instance)

# Save force plot as HTML
force_plot = shap.force_plot(explainer.expected_value, shap_values, selected_instance)
shap.save_html("shap_force_plot_sample.html", force_plot)

print("Force plot saved as: shap_force_plot_sample.html")

!pip install selenium webdriver-manager

driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)

!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!sudo apt install ./google-chrome-stable_current_amd64.deb -y

# Install necessary libraries for Selenium if not already installed
!pip install selenium webdriver-manager

# Install a headless version of Chrome
!apt-get update
!apt-get install -y google-chrome-stable --no-install-recommends

import shap
import random
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service # Import Service
import time
import os # Import os module for abspath

# --- Generate SHAP force plot and save as HTML ---
X_test_sampled = X_test.sample(n=100, random_state=42)
sample_index = random.randint(0, 99)
selected_instance = X_test_sampled.iloc[sample_index:sample_index+1]

explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(selected_instance)
force_plot = shap.force_plot(explainer.expected_value, shap_values, selected_instance)
shap.save_html("shap_force_plot_sample.html", force_plot)

# --- Setup Selenium to take a screenshot of the HTML ---
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--window-size=1200x600")  # You can adjust the size
# Specify the path to the installed Chrome binary
chrome_options.add_argument("--no-sandbox") # Required in some headless environments
chrome_options.add_argument("--disable-dev-shm-usage") # Required in some headless environments


# Use webdriver_manager to install the correct driver and pass it via Service
service = Service(ChromeDriverManager().install()) # Use Service class
driver = webdriver.Chrome(service=service, options=chrome_options) # Pass service and options explicitly

driver.get("file://" + os.path.abspath("shap_force_plot_sample.html"))

# Give JS time to render
time.sleep(3)

# Save screenshot
driver.save_screenshot("shap_force_plot_sample.png")
driver.quit()

print("SHAP force plot saved as PNG: shap_force_plot_sample.png")

# Install necessary libraries for Selenium if not already installed
!pip install selenium webdriver-manager

import shap
import random
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
import time
import os # Import os module for abspath

# --- Generate SHAP force plot and save as HTML ---
X_test_sampled = X_test.sample(n=100, random_state=42)
sample_index = random.randint(0, 99)
selected_instance = X_test_sampled.iloc[sample_index:sample_index+1]

explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(selected_instance)
force_plot = shap.force_plot(explainer.expected_value, shap_values, selected_instance)
shap.save_html("shap_force_plot_sample.html", force_plot)

# --- Setup Selenium to take a screenshot of the HTML ---
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--window-size=1200x600")  # You can adjust the size

# Use webdriver_manager to install the correct driver
driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)
driver.get("file://" + os.path.abspath("shap_force_plot_sample.html"))

# Give JS time to render
time.sleep(3)

# Save screenshot
driver.save_screenshot("shap_force_plot_sample.png")
driver.quit()

print("SHAP force plot saved as PNG: shap_force_plot_sample.png")

pip install shap --upgrade



import shap
import pandas as pd
import matplotlib.pyplot as plt

# Use a sample of the test set for speed (optional)
X_sample = X_test.sample(n=1000, random_state=42) if len(X_test) > 1000 else X_test

# Create a SHAP Explainer for RandomForest
explainer = shap.Explainer(best_model, X_sample)

# Compute SHAP values for the entire sample
shap_values = explainer(X_sample)

import shap
import pandas as pd
import matplotlib.pyplot as plt

# Make sure X_test is a DataFrame with proper column names
X_test = pd.DataFrame(X_test, columns=feature_names)  # replace `feature_names` if needed

# Take a sample if large (optional)
X_sample = X_test.sample(n=1000, random_state=42) if len(X_test) > 1000 else X_test

# Use shap.Explainer for modern behavior (safer than TreeExplainer)
explainer = shap.Explainer(best_model, X_sample)

# Get SHAP values (returns Explanation object with .values, .data, .feature_names)
shap_values = explainer(X_sample)

# Plot feature importance
shap.plots.bar(shap_values, max_display=20)

# Beeswarm plot to show distribution and impact direction
shap.plots.beeswarm(shap_values, max_display=20)

shap.plots.waterfall(shap.Explanation(values=shap_values[0],
                                      base_values=explainer.expected_value,
                                      data=selected_instance.values[0],
                                      feature_names=selected_instance.columns))

import pandas as pd

# Load your data
# df = pd.read_csv('your_file.csv')  # Uncomment and update with your actual file if needed

# Print count of categories under Residency
print(df['Residency'].value_counts())

import pandas as pd

# Example transformation
df['Residency'] = df['Residency'].apply(lambda x: x if x in [1, 2] else 3)

# Optional: check the updated counts
print(df['Residency'].value_counts())

# Crosstab of Residency by DM
cross_tab = pd.crosstab(df['Residency'], df['DM'], margins=True, margins_name='Total')

# Percentages (row-wise)
percent_tab = pd.crosstab(df['Residency'], df['DM'], normalize='index') * 100

# Combine counts and percentages
combined = cross_tab.drop('Total').copy()
for col in cross_tab.columns[:-1]:  # Exclude 'Total' column
    combined[col] = combined[col].astype(str) + ' (' + percent_tab[col].round(1).astype(str) + '%)'

# Print the table
print(combined)

shap.plots.waterfall(shap.Explanation(values=shap_values[0],
                                      base_values=explainer.expected_value,
                                      data=selected_instance.values[0],
                                      feature_names=selected_instance.columns))

import matplotlib.pyplot as plt

plt.tight_layout()
plt.savefig("shap_waterfall_plot_sample.png", dpi=300)
plt.show()





# Alternative bar plot for static explanation
# Pass the selected_instance DataFrame directly as the features argument
shap.plots.bar(shap.Explanation(values=shap_values_sample[0],
                                base_values=explainer_sample.expected_value,
                                # Change data from 2D array (1, 23) to 1D array (23,)
                                data=selected_instance.values[0], # Use .values[0] to get the 1D array
                                feature_names=selected_instance.columns.tolist()),
               show_data=True # Ensure data values are shown on the bars if desired
              )

# A potentially simpler way, often sufficient for single instances:
# Directly pass shap_values_sample and the corresponding instance DataFrame
# This allows SHAP's plotting functions to handle the data and names internally
# Keep this one as it's likely to work without modification
# Removed the line that caused the error: shap.plots.bar(shap_values_sample[0], feature_names=selected_instance.columns.tolist(), show_data=True)

import shap

# Use TreeExplainer for XGBoost
explainer = shap.Explainer(models["XGBoost"])  # auto-selects TreeExplainer internally
shap_values = explainer(X_test)  # returns shap.Explanation object

# Now plot
shap.plots.beeswarm(shap_values)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap

# Explain and get SHAP values
explainer = shap.Explainer(models["XGBoost"])
shap_values = explainer(X_test)
shap_df = pd.DataFrame(shap_values.values, columns=shap_values.feature_names)

# Optional: also get feature values
feature_values = pd.DataFrame(shap_values.data, columns=shap_values.feature_names)

# Melt for beeswarm-style plot
shap_long = shap_df.melt(var_name='feature', value_name='shap_value')
feature_long = feature_values.melt(var_name='feature', value_name='feature_value')

# Combine both
plot_df = shap_long.copy()
plot_df['feature_value'] = feature_long['feature_value']

# Plot with seaborn
plt.figure(figsize=(12, 8))
ax = sns.stripplot(data=plot_df, x='shap_value', y='feature', jitter=True, alpha=0.6)

# Add numbers (only a sample to avoid clutter)
for i, row in plot_df.sample(n=50, random_state=42).iterrows():  # limit annotations
    ax.text(row['shap_value'], row['feature'], f"{row['feature_value']:.2f}",
            fontsize=8, alpha=0.8, color='black')

plt.title("SHAP Beeswarm Plot with Feature Values")
plt.tight_layout()
plt.show()



import matplotlib.pyplot as plt

shap.plots.beeswarm(shap_values, show=False)
plt.tight_layout()
plt.show()

import shap

# Compute SHAP values
explainer = shap.Explainer(models["XGBoost"])
shap_values = explainer(X_test)

# Beeswarm plot showing top N features
shap.plots.beeswarm(shap_values, max_display=15)  # Change 10 to your desired number

shap.plots.bar(shap_values, max_display = 24)



print("\n--- Final Model Performance Summary ---")
print(results_df.to_markdown(index=False))



from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Ensure AUC is numeric for sorting, handling 'N/A'
results_df['AUC_numeric'] = pd.to_numeric(results_df['AUC'], errors='coerce')

# Sort by AUC descending, then Accuracy descending
sorted_results = results_df.sort_values(by=['AUC_numeric', 'Accuracy'], ascending=[False, False])

# Get the name of the best model
best_model_name = sorted_results.iloc[0]['Model']

print(f"\n--- Confusion Matrix for Best Model: {best_model_name} ---")

# --- Re-train the best model to get predictions ---
# We need to re-instantiate and train the best model to get its predictions
# This is because the predictions from the loop were not stored individually for each model
# (Except for probabilities in model_proba_dict for AUC plotting and threshold tuning)

# Find the best model from the original models dictionary
best_model = models[best_model_name]

# Train the best model on the SMOTE-augmented data
best_model.fit(X_train_smote, y_train_smote)

# Get predictions on the original test set
y_pred_best_model = best_model.predict(X_test)

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred_best_model)

# Plot the confusion matrix using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title(f'Confusion Matrix for {best_model_name}')
plt.show()

print("Confusion Matrix:")
print(cm)



import pandas as pd

# Define the output Excel filename
excel_filename = 'formatted_crosstab_DM.xlsx'

# Assume 'DM' is your target variable
target_col = 'DM'

# Get the list of all columns except the target
feature_cols = [col for col in df.columns if col != target_col]

# Create a list to hold the final formatted tables
final_rows = []

for feature in feature_cols:
    # Get crosstab counts
    counts = pd.crosstab(df[feature], df[target_col], dropna=False)

    # Get crosstab row-wise percentages
    percentages = pd.crosstab(df[feature], df[target_col], normalize='index', dropna=False) * 100

    # Combine counts and percentages into one string: e.g. "2010 (29.5%)"
    combined = counts.astype(str) + ' (' + percentages.round(1).astype(str) + '%)'

    # Reset index to get the category as a column
    combined.reset_index(inplace=True)

    # Insert the variable name only in the first row of the block
    combined.insert(0, 'Variable', feature)
    combined.loc[1:, 'Variable'] = ''  # Leave the rest of the block blank

    # Rename the category column for clarity
    combined.rename(columns={feature: 'Category'}, inplace=True)

    # Append to final result list
    final_rows.append(combined)

# Concatenate all formatted tables
final_table = pd.concat(final_rows, ignore_index=True)

# Save to Excel
final_table.to_excel(excel_filename, index=False)

print(f"✅ Cross-tabulation saved to '{excel_filename}' in the requested format.")



import pandas as pd

# Define the output Excel filename
excel_filename = 'formatted_crosstab_htn.xlsx'

# Assume 'DM' is your target variable
target_col = 'hypertension_avg'

# Get the list of all columns except the target
feature_cols = [col for col in df.columns if col != target_col]

# Create a list to hold the final formatted tables
final_rows = []

for feature in feature_cols:
    # Get crosstab counts
    counts = pd.crosstab(df[feature], df[target_col], dropna=False)

    # Get crosstab row-wise percentages
    percentages = pd.crosstab(df[feature], df[target_col], normalize='index', dropna=False) * 100

    # Combine counts and percentages into one string: e.g. "2010 (29.5%)"
    combined = counts.astype(str) + ' (' + percentages.round(1).astype(str) + '%)'

    # Reset index to get the category as a column
    combined.reset_index(inplace=True)

    # Insert the variable name only in the first row of the block
    combined.insert(0, 'Variable', feature)
    combined.loc[1:, 'Variable'] = ''  # Leave the rest of the block blank

    # Rename the category column for clarity
    combined.rename(columns={feature: 'Category'}, inplace=True)

    # Append to final result list
    final_rows.append(combined)

# Concatenate all formatted tables
final_table = pd.concat(final_rows, ignore_index=True)

# Save to Excel
final_table.to_excel(excel_filename, index=False)

print(f"✅ Cross-tabulation saved to '{excel_filename}' in the requested format.")

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import pandas as pd # Import pandas if not already imported in this cell
from sklearn.preprocessing import LabelEncoder # Import LabelEncoder

# Your full features and target
# Ensure df is the DataFrame you intend to use at this point
# If df was reloaded or modified, make sure it reflects the state you want
X = df.drop('DM', axis=1)
y = df['DM']

# --- Add preprocessing steps here ---
# Identify non-numeric columns in X *before* applying SMOTE
categorical_cols_in_X = X.select_dtypes(include='object').columns

# Apply Label Encoding to remaining categorical columns in X
# Create a copy to avoid SettingWithCopyWarning
X_processed = X.copy()
for col in categorical_cols_in_X:
    print(f"Encoding column: {col}") # Optional: print which columns are being encoded
    le = LabelEncoder()
    # Fit and transform the column
    X_processed.loc[:, col] = le.fit_transform(X_processed[col])

# It's also good practice to ensure the target variable y is numeric (usually 0 or 1 for binary classification)
# Assuming 'DM' is already binary (0 or 1) or 'yes'/'No' and was encoded earlier, this might not be strictly necessary,
# but casting to int ensures it's numeric if it's not already.
# Check the dtype of y before this if unsure.
# y = y.astype(int) # Uncomment if your target is not already integer/numeric

# --- Apply SMOTE to the processed data ---
smote = SMOTE(random_state=42)
# Use X_processed instead of X
X_resampled, y_resampled = smote.fit_resample(X_processed, y)

# Now split the resampled data into train and test
# Stratify by y_resampled to maintain class distribution in the split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

# Print class distribution
print("\nAfter SMOTE (then split):")
print("Train class distribution:\n", y_train.value_counts())
print("Test class distribution:\n", y_test.value_counts())

# %%
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import numpy as np  # Needed for checking unique y values

# Your dataset
# df = ... # Ensure df is already defined

# Separate features and target
X = df.drop('DM', axis=1)
y = df['DM']

# Encode categorical features
categorical_cols = X.select_dtypes(include='object').columns
X_encoded = X.copy()

for col in categorical_cols:
    le = LabelEncoder()
    X_encoded[col] = le.fit_transform(X_encoded[col])

# Apply SMOTE to the full dataset
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_encoded, y)

# Train-test split on resampled data
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

# %%
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score

# Define models
models = {
    "Logistic Regression": LogisticRegression(random_state=42, solver='liblinear'),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Gaussian Naive Bayes": GaussianNB(),
    "SVM (Linear)": SVC(kernel='linear', random_state=42, probability=True),
    "SVM (RBF)": SVC(kernel='rbf', random_state=42, probability=True),
    "MLP Classifier": MLPClassifier(random_state=42, max_iter=1000),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42)
}

# Initialize results DataFrame
results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'])

# Train and evaluate
for model_name, model in models.items():
    print(f"Training {model_name}...")

    # Train
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_test)

    # Probabilities for AUC
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else 'N/A'

    # Append to results
    results_df = pd.concat([results_df, pd.DataFrame([{
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'AUC': auc
    }])], ignore_index=True)

# Summary
print("\n--- Model Performance Summary ---")
print(results_df.to_markdown(index=False))

